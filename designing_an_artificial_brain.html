<h1>
Designing an artificial intelligence:<br/>
An introduction to Bayesian statistics
</h1>

<h2>
Setting up
</h2>

<p>Drawing conclusions is relatively straightforward when you're sure of what
you know.  A is true, and A implies B, therefore B is true.  Here A and B are
<strong>propositions</strong>, statements which are (in principle) either true or false,
even if we have no way of knowing which of the two they are.
</p>

<p>However, most decisions that we need to make depend
on propositions that can't be judged true or false given the information at
hand.  Is it going to rain today?  Will I get sick if I don't get a flu
vaccine?  Will the stock market go up or down in six months? If we knew the
answers to these questions ahead of time, even approximately, it would
allow us to make better decisions.
</p>

<p>Is it possible then, to design an artificial intelligence (an AI) to help us
reason about uncertain propositions in some sort of optimal way?  In order
to begin, we would first need to come up with a relatively satisfying
definition of 'optimal'.  At first one might hope for an AI that
that provides some sort of guarantee of the correctness of the answers it provides.  As far
as I know this hasn't been done (after all, we are dealing
with <em>uncertain</em> propositions), but there has been progress towards
satisfying the following 4 criteria:
</p>

<ol>
<li>
Universality: we would like our AI to be able to provide answers to a
wide variety of questions.
</li>

<li>
Simplicity: as a practical matter, a simple design is easier to understand and implement.
</li>

<li>
Agrees with our intuition: I can't be more precise about this until I get
into the details of our AI's internals, but don't worry, I will be very
specific when I invoke this requirement.
</li>

<li>
Consistency: we would like for our AI to give us the same answer
every time we ask it a specific question.
</li>
</ol>

<p>
Of the four of these criteria, the last one, consistency, seems to me the
least important.  If our AI is not universal it will not be able to provide
an answer to many of our questions; if it is not simple it will also not be
able to provide many answers, because it will be too difficult to implement
and improve upon; and if it doesn't agree with our intuition then the
answers it does give us will appear absurd or arbitrary.  However, a lack of
consistency in the answers it gives us doesn't result in any similar
disadvantage.
</p>

<p>
This may seem strange, since normally if we are designing a machine to
evaluate propositions a lack of consistency is an indication of a
serious error.  For example, if you had a calculator that gave a different
answer every time you typed in '2308 * 5982' then (even not knowing the
answer yourself) you would be able to tell there is something wrong with the
calculator, since there is only one number equal to 2308 * 5982, and it can
be determined based on the given data.
</p>

<p>
However, imagine that the purpose of your calculator was to determine the
mass of Saturn based on a series of astronomical observations. In that case,
we can't say that something is wrong with the calculator because it doesn't
output the same answer every time we input the same exact data since
(in the context of this example) the data given to the calculator is not sufficient to
determine the actual mass of Saturn.
</p>

<p>
However, there is at least one practical reason to desire consistency;
debugging our AI will be easier when we know exactly what response to expect
from it based on a given input.  So although I will not stress the
importance of constistency, I will mention in passing how we can ensure our
AI will provide consistent answers, at least for debugging purposes.
</p>

<h2>
AI internals
</h2>

When human beings reason about uncertain propositions, we often attach
weights or 'levels of belief' to the different propositions under
consideration.  For example, if there are clouds in the sky one might think
of the proposition 'it is going to rain today' as having more weight than
the negation of that statement, 'it is not going to rain today'.
</p>

<p>
This suggests one way to design our AI. Given an uncertain proposition A
which the AI must decide on and some data I related to A, the AI could attach a real
number to A, and another real number to the negation of A, -A.  Since this
idea is inspired by the way humans assign levels of belief to uncertain
propositions, we will call the first number B(A | I), which you can
pronounce 'the AI's belief in A given background information I', and the second number B(-A |
I).  If B(A | I) >= B(-A | I), then the AI's output will be 'A is true', and
otherwise 'A is false'.  As you can see, the robot displays a preference for
the original proposition A when the beliefs are equal. We could just as
easily have had things the other way, but since the robot must make some
decision we might as well choose one arbitrarily (here you can see the
consistency issue already begin to rear its head).
</p>

<p>
It is important to understand the following point: the AI's level of belief
B(A | I) in the proposition A is only a description of the AI's internal
state; it has no significance beyond the internals of our AI.  It is not
a statement about the true nature of the proposition A, nor is it a statement about the long
term frequency of A being true in repeated trials, whatever that would mean.
It is merely a description of the AI's internals; any attempt to attach
additional significance to it will only result in confusion.
</p>

<p>
It is also worth mentioning that this is not the only way to design such an
AI; instead of attaching one real number to a given proposition we could
attach several, or we could imagine some other mechanism which doesn't
involve numbers at all.  This idea merely suggests itself to us because it
is simple, and it is similar to the way we humans think.
</p>

<p>
The only question now is, how will our AI determine B(A | I) in a way that
is both simple and agrees with our intuition?  Let's state some other design
principles for the internals of our AI, starting with the principle just mentioned:
</p>

<p>
D1) Given a proposition A and some background information I, the AI will assign a real number
B(A | I).  This is similar to the way that humans reason about uncertain
propositions.
</p>

<p>
D2) There exists some real number T such that B(A | I) <= T for every A and
I.  If A is known to be true, then the AI should assign B(A | I) = T.
</p>

<p>
Logically this is not essential, but storage space constraints mean that
there currently isn't a computer that can store any real number, so without
this requirement we would be stuck.
</p>

<p>
D3) The assignment of beliefs is compatible with everyday logic.
</p>

<p>
This is part of our 3rd criterium; the AI must assign beliefs to propositions in a way that
agrees with our intuition. For example, if A and A' are equivalent
propositions then the AI should assign equal levels of belief to A and A',
so that B(A | I) = B(A' | I).  There are some other requirements here, but
they are all similarly straightforward, see Kevin van Horn for details.
</p>

<p>
D4) There is a nonincreasing function S such that S(B(A | I)) = B(-A | I).
Define F = S(T).
</p>

<p>
This also is meant to agree with our intution.  In simple English, it means
that if some new data causes the AI to assign a higher level of belief to A,
then it's level of belief in the negation of A should not increase as well.
</p>

<p>
D5) There is no such thing as an 'atom of evidence' for a proposition; that
is, there are no intervals contained in (F,T) that contain belief values
which are not possible for our robot to have (this is not technically
correct, see van Horn for details).
</p>

<p>
The primary justification for this requirement is that it appeals to our
intuition, since we humans do not think of levels of belief as being finite
and discrete, we think of them as being continuous. It is not natural to
imagine that given some information I related to A, and some more favorable information I' for
A, there is not some intermediate level of information I'' that would allow the AI to have
B(A | I) < B(A | I'') < B(A | I')
</p>

<p>
D6) There is a strictly increasing, continuous function F such that
B(A ^ C | I) = F(B(A | C, I), B(C | I)) for all propositions A and C with
background information I.
</p>

<p>
Once again, the argument for this requirement is that it appeals to our
common sense.  One's belief in a both A and C should depend on both one's
belief in C, and one's belief in A given that C is true.  See van Horn for
further arguments in favor of this requirement.
</p>

<h2>Bayes' Rule</h2>

<p>
That's it, we have set out 5 requirements for how our AI should operatore
internally, and that is enough to completely determine the algorithm that
our AI will use to calculate belief values for every problem it could
encounter. For more information on this result, known as Cox's Theorem, see van Horn's awesome
paper.
</p>

<p>
Without going into the proof, it turns out that that the function B can be
rescaled so that B(T) = 1 and B(F) = 0, matching the usual mapping of 1 to
true propositions, and 0 to false propositions.  It can also be shown that
the following two rules hold:
</p>

<p>
The sum rule:
B(-A | I) = 1 - B(A | I)
</p>

<p>
The product rule:
B(A ^ C | I) = B(A | C, I) B(C | I)
</p>

<p>
Now if we switch the roles of A and C in the product rule, we have
</p>

<p>
B(C ^ A | I) = B(C | A, I) B(A | I)
</p>

<p>
but B(C ^ A | I) = B(A ^ C | I) by D1, since A ^ C and C ^ A are equivalent
propositions, so we have that
</p>

<p>
B(A | C, I) B(C | I) = B(C | A, I) B(A | I)
</p>

<p>
Finally, by dividing both sides by B(C | I), we have the Bayes'-Laplace Rule
(traditionally shortened to Bayes' Rule):
</p>

<p>
B(A | C, I) = [ B(C | A, I) B(A | I) ] / B(C | I)
</p>

<p>
If we let A be our hypothesis, and C be the data we have collected
concerning the hypothesis, then we can see the true power of Bayes' rule.
</p>

<p>
B(hypothesis | data, I) =
[ B(data | hypothesis, I) B(hypothesis | I) ] / B(data | I)
</p>

<p>
The term B(hypothesis | data, I) is traditionally called the <strong>posterior</strong>,
since it describes our AI's belief in the hypothesis <em>after</em> viewing the
data.  The term B(data | hypothesis, I) is called the <strong>evidence</strong> or more
traditionally the <strong>likelihood</strong>, it assigns a number to the degree to which the
hypothesis is supported by the data.  B(hypothesis | I) is the <strong>prior</strong>, it
shows our AI's belief in the hypothesis <em>before</em> viewing the data.  B(data |
I) is simply a constant which does not depend on the hypothesis, so it is
often ignored since we are usually only interested in choosing a good hypothesis.
</p>

<p>
One of the reasons that Bayes' Rule is so celebrated is because it precisely
describes the scientific method--the idea that we should use data to update
our beliefs about a hypothesis. Because of this simple equation our AI has
almost unlimited applicability, since so many problems that we encounter can
be described as 'updating a hypothesis in light of the data'.
</p>

<p>
It is worth reiterating an earlier point; there is no claim here that this
is the only way to reason about uncertain propositions, or even the best
way. It is merely a simple, widely applicable, and intuitively appealing
way for humans (and machines designed by humans) to make decisions.  While
Bayes' Rule is elegant and highly appealing in its own right, it
could not be described as a universal law in any meaningful way; it is just
a highly idealized description of the way humans reason about
uncertain propositions.
</p>
