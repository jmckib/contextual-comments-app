<h1>
Designing an artificial intelligence:<br/>
An introduction to Bayesian statistics
</h1>

<h2>
Setting up
</h2>

<p>Drawing conclusions is relatively straightforward when you're sure of what
you know.  A is true, and A implies B, therefore B is true.  Here A and B are
<strong>propositions</strong>, statements which are (in principle) either true or false,
even if we have no way of knowing which of the two they are.
</p>

<p>However, the vast majority of decisions that we need to make depend on
propositions that we cannot judge to be definitively true or false given the
information at hand.  Is it going to rain today?  Will I get sick if I don't
get a flu vaccine?  Will the stock market go up or down in six months? If we
knew the answers to these questions ahead of time, even approximately, it
would enable us to make better decisions.
</p>

<p>Is it possible then, to design an artificial intelligence (an AI) to help us
reason about uncertain propositions in some sort of optimal way?  In order
to begin, we would first need to come up with a relatively satisfying
definition of 'optimal'.  At first one might hope to design an AI that
is correct more often than any other AI that could be designed.  As far
as I know this has not been done, but there has been progress towards
satisfying the following 4 criteria:
</p>

<ol>
<li>
1). Universality: we would like our AI to be able to provide answers to a
wide variety of questions.
</li>

<li>
2). Simplicity: as a practical matter, a simple design is easier to understand and implement.
</li>

<li>
3). Agrees with our intuition: I can't be more precise about this until
later, when I get into the details of the design of our AI, but don't worry,
I will be very specific when I invoke this requirement.
</li>

<li>
4). Consistency: we would like for our AI to give us the same answer
every time we ask it a specific question.
</li>
</ol>

<p>
Of the four of these criteria, the last one, consistency, seems to me the
least important.  If our AI is not universal it will not be able to provide
an answer to many of our questions; if it is not simple it will also not be
able to provide many answers, because it will be too difficult to implement
and improve upon; and if it doesn't agree with our intuition then the
answers it does give us will appear absurd or arbitrary.  However, a lack of
consistency in the answers it gives us doesn't result in any similar
disadvantage.
</p>

<p>
This may seem strange, since normally if we are designing a machine to
evaluate propositions then a lack of consistency is an indication of a
serious error.  For example, if you had a calculator that gave a different
answer every time you typed in '2308 * 5982' then (even not knowing the
answer yourself) you could tell there is something wrong with the
calculator, since there is only one number equal to 2308 * 5982, and it can
be determined based on the given data.
</p>

<p>
However, imagine that the purpose of your calculator was to determine the
mass of Saturn based on a series of astronomical observations. In this case,
we can't say that something is wrong with the calculator because it doesn't
output the same answer every time we ask, even if we provide it with the
exact same data each time, since (hypothetically) the data given to the
calculator is not sufficient to determine the actual mass of Saturn.
</p>

<p>
However, there is at least one practical reason to desire consistency;
debugging our AI will be easier when we know exactly what response to expect
from it based on a given input.  So although I will not stress the
importance of constistency, I will mention in passing how we can ensure our
AI will provide consistent answers, at least for debugging purposes.
</p>

<h2>
AI internals
</h2>

<p>
In order to explain how our AI will work internally I will need to be more
specific about one kind of question our AI might attempt to answer, but I
will show later that the design used here can be applied to many types of
questions.
</p>

<p>
Suppose that there is some uncertain proposition A, and we have collected
observations from the real world that are somehow related to A (I will be
more specific about this relationship later).  We would like our AI to tell
us that A is either true or false.  For example, we might flip a coin 10
times, and then ask our AI to tell us if the 11th flip will be heads or not.
</p>

When human beings reason about uncertain propositions, we often attach
weights or 'levels of belief' to the different propositions under
consideration.  For example, one might think of the proposition 'it is going
to rain today' as having more weight than the negation of that statement,
'it is not going to rain today'.
</p>

<p>
This suggests one way to design our AI. Given an uncertain proposition A
which it must decide on and some data D related to A, it could attach a real
number to A, and another real number to the negation of A, -A.  Since this
idea is inspired by the way humans assign levels of belief to uncertain
propositions, we will call the first number B(A | I), which you can
pronounce 'the AI's belief in A given background information I', and the second number B(-A |
I).  If B(A | I) >= B(-A | I), then the AI's output will be 'A is true', and
otherwise 'A is false'.  As you can see, the robot displays a preference for
the original proposition A when the beliefs are equal. We could just as
easily have had things the other way, but since the robot must make some
decision we might as well choose on arbitrarily (here you can see the
consistency issue already begin to rear its head).
</p>

<p>
It is important to understand the following point: the AI's level of belief
B(A | I) in the proposition A is only a description of the AI's internal
state; it has no significance beyond the internals of our AI.  It is not
a statement about the true nature of A, nor is it a statement about the long
term frequence of A being true in repeated trials, whatever that would mean.
It is merely a description of the AI's internals, any attempt to attach
additional significance to it will only result in confusion.
</p>

<p>
It is also worth mentioning that this is not the only way to design such an
AI; instead of attaching one real number to a given proposition we could
attach several, or we could imagine some other mechanism which doesn't
involve numbers at all.  This idea merely suggests itself to us because it
is simple, and it is similar to the way we think.
</p>

<p>
Also, as far as I know, there is also no proof that this design will result
in an AI that provides correct answers more often than any other method
(although I don't deny that some kind of proof might be possible; I just
haven't seen one yet).
</p>

<p>
The only question now is, how will our AI determine B(A | I) in a way that
is both simple and agrees with our intuition?  Let's state some other design
principles for the internals of our AI, starting with the principle just mentioned:
</p>

<p>
D1) Given a proposition A and some data D, the AI will assign a real number
B(A | I).  This is similar to the way that humans reason about uncertain
propositions.
</p>

<p>
D2) There exists some real number T such that B(A | I) <= T for every D and
A.  If A is known to be true, then the AI should assign B(A | I) = T.
</p>

<p>
Logically this is not essential, but storage space constraints mean that
there currently isn't a computer that can store any real number, so without
this requirement we would be stuck.
</p>

<p>
D3) The assignment of beliefs is compatible with everyday logic.
</p>

<p>
This is part of our 3rd design principle; the AI must make act in a way that
agrees with our intuition. For example, if A and A' are equivalent
propositions then the AI should assign equal levels of beliefe to A and A',
i.e., B(A | I) = B(A' | I).  There are some other requirements here, but
they are all similarly straightforward, see Kevin van Horn for details.
</p>

<p>
D4) There is a nonincreasing function S such that S(B(A | I)) = B(-A | I).
Define F = S(T).
</p>

<p>
This also is meant to agree with our intution.  In simple English, it means
that if some new data causes the AI to assign a higher level of belief to A,
then it's level of belief in -A should not increase as well.
</p>

<p>
D5) There is no such thing as an 'atom of evidence' for a proposition; that
is, there are no intervals contained in (F,T) that contain belief values
which are not possible for our robot to have (this is not technically
correct, see van Horn for details).
</p>

<p>
The primary justification for this requirement is that it appeals to our
intuition, since we humans do not think of levels of belief as being finite
and discrete, we think of them as being continuous. It is not natural to
imagine that given some data D for A, and some more convincing data D' for
A, there is not some intermediate data that would allow the AI to have
B(A | I) < B(A | I'') < B(A | I')
</p>

<p>
D5) There is a strictly increasing function, continuous function F such that
B(A ^ C | I) = F(B(A | C, D), B(C | I)) for all propositions A and C with
data D.
</p>

<p>
Once again, the argument for this requirement is that it appeals to our
common sense.  One's belief in a both A and C should depend on both one's
belief in C, and one's belief in A given that C is true.  See van Horn for
further arguments in favor of this requirement.
</p>

<h2>Bayes' Rule</h2>

<p>
That's it, we have set out 5 requirements for how our AI should operatore
internally, and that is enough to completely determine the algorithm that
our AI will use to calculate belief values for every problem it could
encounter. For more information on this result, see van Horn's awesome
paper.
</p>

<p>
Without going into the proof, it turns out that that the function B can be
rescaled so that B(T) = 1 and B(F) = 0, matching the usual mapping of 1 to
true propositions, and 0 to false propositions.  It can also be shown that
the following two rules hold:
</p>

<p>
The sum rule:
B(-A | I) = 1 - B(A | I)
</p>

<p>
The product rule:
B(A ^ C | I) = B(A | C, I) B(C | I)
</p>

<p>
Now if we switch the roles of A and C in the product rule, we have
</p>

<p>
B(C ^ A | I) = B(C | A, I) B(A | I)
</p>

<p>
but B(C ^ A | I) = B(A ^ C | I) by D1, since A ^ C and C ^ A are equivalent
propositions, so we have that
</p>

<p>
B(A | C, I) B(C | I) = B(C | A, I) B(A | I)
</p>

<p>
Finally, by dividing both sides by B(C | I), we have the Bayes'-Laplace Rule
(traditionally shortened to Bayes' Rule):
</p>

<p>
B(A | C, I) = [ B(C | A, I) B(A | I) ] / B(C | I)
</p>

<p>
If we let A be our hypothesis, and C be the data we have collected
concerning the hypothesis, then we can see the true power of Bayes' rule.
</p>

<p>
B(hypothesis | data, I) =
[ B(data | hypothesis, I) B(hypothesis | I) ] / B(data | I)
</p>

<p>
The term B(hypothesis | data, I) is traditionally called the <strong>posterior</strong>,
since it describes our AI's belief in the hypothesis <em>after</em> viewing the
data.  The term B(data | hypothesis, I) is called the <strong>evidence</strong> or more
traditionally the <strong>likelihood</strong>, it assigns a number to the degree to which the
hypothesis is supported by the data.  B(hypothesis | I) is the <strong>prior</strong>, it
shows our AI's belief in the hypothesis <em>before</em> viewing the data.  B(data |
I) is simply a constant which does not depend on the hypothesis, so it is
usually ignored, since we are usually interested in choosing a good hypothesis.
</p>

<p>
The reason that Bayes' Rule is so celebrated is because it precisely
expresses the scientific method--the idea that we should use data to update
our beliefs about a hypothesis. It means that our AI has almost unlimited
applicability, since so many problems that we encounter can be described as
'updating a hypothesis in light of the data'.
</p>

<p>
It is worth reiterating an earlier point; there is no claim here that this
is the only way to reason about uncertain propositions, or even the best
way. It is merely an simple, widely applicable, and intuitively appealing
way for humans (and machines designed by humans) to make decisions.  While
the design just shown is elegant and highly appealing in its own right, it
could not be described as a universal law in any meaningful way; it is just
a highly idealized description of the way humans think.
</p>
